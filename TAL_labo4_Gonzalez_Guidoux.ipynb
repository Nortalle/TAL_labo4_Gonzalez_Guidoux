{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours TAL – Labo 4\n",
    "Nathan Gonzalez Montes et Vincent Guidoux\n",
    "\n",
    "## Exercice 1 - exécuter la NER dans NLTK\n",
    "\n",
    "### En utilisant nltk.ne_chunk (voir livre NLTK p. 283 - 4),extraire les entités nommées les plus fréquentes  d’un texte de votre choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importation des librairies nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vincent\n",
      "[nltk_data]     Guidoux\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vincent Guidoux\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Vincent\n",
      "[nltk_data]     Guidoux\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Vincent\n",
      "[nltk_data]     Guidoux\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import nltk\n",
    "import os, codecs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk import FreqDist\n",
    "from urllib import request\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importation du livre et nous prenons que la partie intéressante de celui-ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"http://www.gutenberg.org/files/18488/18488.txt\"\n",
    "\n",
    "response = request.urlopen(url1)\n",
    "raw = response.read().decode('utf8')[4340:489067]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisation en phrases, et des phrases en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6701"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(raw)\n",
    "\n",
    "sentences_of_words = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "len(sentences_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous taguons les mots dans chaque phrase grace à NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6701"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in sentences_of_words]\n",
    "    \n",
    "len(tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grace à `ne_chunk` les phrases sont transformées en arbre et les entités nommées sont mise en évidence grace au label qui nous permet de les retrouver et de les stocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for chunk in ne_chunk(sentence):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            trees.append((chunk.label(), ' '.join(c[0] for c in chunk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous en sortons les 15 entités nommées les plus fréquentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_most_commons = FreqDist(trees).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En vous inspirant du POS de Stanford, utiliser leur NER (https://nlp.stanford.edu/software/CRF-NER.html) et la classe StanfordNERTagger de NLTK, pour extraire les entités nommées du même texte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importation du `StanfordNERTagger` et tokenization des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106728"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "st = StanfordNERTagger('stanford/classifiers/english.all.3class.distsim.crf.ser.gz', \n",
    "                       'stanford/stanford-ner.jar', \n",
    "                       encoding='utf-8')\n",
    "sentences = nltk.word_tokenize(raw)\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taguage des mots pour trouver les entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CHAPTER', 'O'),\n",
       " ('I', 'O'),\n",
       " ('Priscilla', 'PERSON'),\n",
       " ('Glenn', 'PERSON'),\n",
       " ('stood', 'O'),\n",
       " ('on', 'O'),\n",
       " ('the', 'O'),\n",
       " ('little', 'O'),\n",
       " ('slope', 'O'),\n",
       " ('leading', 'O')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tagged_sentences = [st.tag(sentence) for sentence in sentences]\n",
    "tagged_sentences = st.tag(sentences)\n",
    "tagged_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Il faut désormais merger le mots tagués comme `('Priscilla', 'PERSON')` et `('Glenn', 'PERSON')` car `StanfordNERTagger`ne le fait pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PERSON', 'Priscilla Glenn'),\n",
       " ('ORGANIZATION', 'Kenmore'),\n",
       " ('ORGANIZATION', 'Across Priscilla'),\n",
       " ('PERSON', 'Nathaniel Glenn'),\n",
       " ('PERSON', 'Nathaniel Glenn'),\n",
       " ('LOCATION', 'Kenmore'),\n",
       " ('PERSON', 'Glenn'),\n",
       " ('PERSON', 'Nathaniel'),\n",
       " ('LOCATION', 'Kenmore'),\n",
       " ('PERSON', 'Glenn')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty = 'O'\n",
    "\n",
    "entities = []\n",
    "\n",
    "last_entity = empty\n",
    "last_word = empty\n",
    "for tagged_word in tagged_sentences: # on parcourd chaque mot tagué\n",
    "    current_entity = tagged_word[1]\n",
    "    current_word = tagged_word[0]\n",
    "\n",
    "    if current_entity != empty: # Si l'entity courante est une PERSON, ORGANIZATION, GPE, ou LOCATION\n",
    "        if last_entity == current_entity: # et quelle est la même que le mot précédent, nous les jumelons\n",
    "            last_word = last_word + ' ' + current_word\n",
    "        else: # on garde en mémoire son entité et le mot\n",
    "            last_entity = current_entity\n",
    "            last_word = current_word\n",
    "    else: # si l'entity courante n'est pas nommée\n",
    "        if last_entity != empty: # et qu'en mémoire il existe un mot tagué\n",
    "            entities.append((last_entity,last_word))\n",
    "            last_entity = empty\n",
    "            last_word = empty\n",
    "        \n",
    "entities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous en sortons les 15 entités nommées les plus fréquentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanfords_most_commons = FreqDist(entities).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparez la liste des NE les plus fréquentes et leurs types reconnus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nltk, stanford)\n",
      "(('PERSON', 'Priscilla'), 227) (('PERSON', 'Priscilla'), 531)\n",
      "(('PERSON', 'Farwell'), 194) (('PERSON', 'Farwell'), 311)\n",
      "(('GPE', 'Priscilla'), 193) (('LOCATION', 'Ledyard'), 135)\n",
      "(('PERSON', 'Ledyard'), 106) (('PERSON', 'Boswell'), 134)\n",
      "(('PERSON', 'Master Farwell'), 64) (('LOCATION', 'Kenmore'), 71)\n",
      "(('ORGANIZATION', 'Boswell'), 61) (('PERSON', 'Nathaniel'), 59)\n",
      "(('ORGANIZATION', 'Travers'), 59) (('PERSON', 'Margaret'), 57)\n",
      "(('PERSON', 'Nathaniel'), 55) (('PERSON', 'Priscilla Glenn'), 54)\n",
      "(('GPE', 'Kenmore'), 55) (('ORGANIZATION', 'Travers'), 53)\n",
      "(('PERSON', 'Margaret'), 52) (('PERSON', 'Dick'), 44)\n",
      "(('GPE', 'Boswell'), 47) (('PERSON', 'McAlpin'), 40)\n",
      "(('PERSON', 'Dick'), 42) (('PERSON', 'Margaret Moffatt'), 39)\n",
      "(('ORGANIZATION', 'Priscilla'), 41) (('PERSON', 'Mary McAdam'), 36)\n",
      "(('ORGANIZATION', 'McAlpin'), 40) (('PERSON', 'Theodora'), 34)\n",
      "(('ORGANIZATION', 'Farwell'), 39) (('LOCATION', 'States'), 34)\n"
     ]
    }
   ],
   "source": [
    "print('(nltk, stanford)')\n",
    "\n",
    "for nltk_freq, stanford in zip(nltk_most_commons[:15], stanfords_most_commons[:15]):\n",
    "    print(nltk_freq, stanford)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priscilla Glenn est le personnage principal de ce roman, il est donc normal de considérer que **Stanford** l'a mieux nommée alors que **nltk** pense à plusieurs reprise que Priscilla Glenn est autre chose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('PERSON', 'Priscilla'), 227)\n",
      "(('GPE', 'Priscilla'), 193)\n",
      "(('ORGANIZATION', 'Priscilla'), 41)\n",
      "(('PERSON', 'Glenn'), 38)\n",
      "(('PERSON', 'Priscilla Glenn'), 31)\n",
      "(('ORGANIZATION', 'Priscilla Glenn'), 13)\n",
      "(('PERSON', 'Nathaniel Glenn'), 4)\n",
      "(('PERSON', 'Priscilla Glynn'), 4)\n",
      "(('ORGANIZATION', 'Glenns'), 2)\n",
      "(('PERSON', 'Mr. Glenn'), 2)\n",
      "(('PERSON', 'Miss Priscilla Glenn'), 2)\n",
      "(('ORGANIZATION', 'Priscilla Glynn'), 2)\n",
      "(('PERSON', 'Glenns'), 1)\n",
      "(('ORGANIZATION', 'Glenn'), 1)\n",
      "(('PERSON', 'Miss Glenn'), 1)\n",
      "(('ORGANIZATION', 'XIV Priscilla Glenn'), 1)\n",
      "(('ORGANIZATION', 'XXIV Priscilla'), 1)\n",
      "(('ORGANIZATION', 'Priscilla Travers'), 1)\n",
      "(('PERSON', 'Priscilla Travers'), 1)\n",
      " - \n",
      "(('PERSON', 'Priscilla'), 531)\n",
      "(('PERSON', 'Priscilla Glenn'), 54)\n",
      "(('PERSON', 'Glenn'), 30)\n",
      "(('PERSON', 'Priscilla Glynn'), 12)\n",
      "(('PERSON', 'Nathaniel Glenn'), 5)\n",
      "(('ORGANIZATION', 'Glenns'), 3)\n",
      "(('PERSON', 'Theodora Glenn'), 3)\n",
      "(('LOCATION', 'Priscilla'), 2)\n",
      "(('ORGANIZATION', 'Across Priscilla'), 1)\n",
      "(('PERSON', 'Miss Glenn'), 1)\n",
      "(('PERSON', 'June Priscilla Glenn'), 1)\n",
      "(('PERSON', '-- Priscilla'), 1)\n",
      "(('ORGANIZATION', 'Priscilla Travers'), 1)\n",
      "(('PERSON', 'Priscilla Travers'), 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for entity in nltk_most_commons:\n",
    "    if \"Priscilla\" in entity[0][1] or \"Glenn\" in entity[0][1] :\n",
    "        print(entity)\n",
    "        \n",
    "\n",
    "print(\" - \")\n",
    "for entity in stanfords_most_commons:\n",
    "    if \"Priscilla\" in entity[0][1] or \"Glenn\" in entity[0][1]:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 - comparer les deux NER sur les données CoNLL2003 (eng.test-a et test-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "NLTK accuracy\n",
      "eng.testa 0.9118034821047734\n",
      "-------------------\n",
      "Stanford accuracy\n",
      "eng.testa 0.9614370468029004\n",
      "-------------------\n",
      "NLTK accuracy\n",
      "eng.testb 0.9002700038571979\n",
      "-------------------\n",
      "Stanford accuracy\n",
      "eng.testb 0.9540779153987914\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.scores import accuracy\n",
    "\n",
    "#source : https://pythonprogramming.net/testing-stanford-ner-taggers-for-accuracy/\n",
    "\n",
    "# Group NE data into tuples\n",
    "def group(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        val = lst[i:i+n]\n",
    "        if len(val) == n:\n",
    "            yield tuple(val)\n",
    "            \n",
    "def nltk_stanfortd_accuracy(filepath):\n",
    "    \n",
    "    raw_annotations = open(filepath).read()\n",
    "    split_annotations = raw_annotations.split()\n",
    "\n",
    "    # Amend class annotations to reflect Stanford's NERTagger\n",
    "    for n,i in enumerate(split_annotations):\n",
    "        if i == \"I-PER\":\n",
    "            split_annotations[n] = \"PERSON\"\n",
    "        if i == \"I-ORG\":\n",
    "            split_annotations[n] = \"ORGANIZATION\"\n",
    "        if i == \"I-LOC\":\n",
    "            split_annotations[n] = \"LOCATION\"\n",
    "\n",
    "    reference_annotations = list(group(split_annotations, 4)) # le fichier de test contient 4 colonnes au lieu de 2 comme dans l'exemple\n",
    "    \n",
    "    #Nous devons prendre la première et dernière colonne\n",
    "    reference_annotations = [(reference_annotation[0],reference_annotation[3]) for reference_annotation in reference_annotations]\n",
    "\n",
    "    # Ok, that looks good! But we'll also need the “clean” form of that data to stick into our NER classifiers. Let's make that happen too.\n",
    "    pure_tokens = split_annotations[::4]\n",
    "    \n",
    "    # Let's go ahead and test the NLTK classifier.\n",
    "    tagged_words = nltk.pos_tag(pure_tokens)\n",
    "    nltk_unformatted_prediction = nltk.ne_chunk(tagged_words)\n",
    "    \n",
    "    #Convert prediction to multiline string and then to list (includes pos tags)\n",
    "    multiline_string = nltk.chunk.tree2conllstr(nltk_unformatted_prediction)\n",
    "    listed_pos_and_ne = multiline_string.split()\n",
    "\n",
    "    # Delete pos tags and rename\n",
    "    del listed_pos_and_ne[1::3]\n",
    "    listed_ne = listed_pos_and_ne\n",
    "\n",
    "    # Amend class annotations for consistency with reference_annotations\n",
    "    for n,i in enumerate(listed_ne):\n",
    "        if i == \"B-PERSON\":\n",
    "            listed_ne[n] = \"PERSON\"\n",
    "        if i == \"I-PERSON\":\n",
    "            listed_ne[n] = \"PERSON\"    \n",
    "        if i == \"B-ORGANIZATION\":\n",
    "            listed_ne[n] = \"ORGANIZATION\"\n",
    "        if i == \"I-ORGANIZATION\":\n",
    "            listed_ne[n] = \"ORGANIZATION\"\n",
    "        if i == \"B-LOCATION\":\n",
    "            listed_ne[n] = \"LOCATION\"\n",
    "        if i == \"I-LOCATION\":\n",
    "            listed_ne[n] = \"LOCATION\"\n",
    "        if i == \"B-GPE\":\n",
    "            listed_ne[n] = \"LOCATION\"\n",
    "        if i == \"I-GPE\":\n",
    "            listed_ne[n] = \"LOCATION\"\n",
    "\n",
    "    # Group prediction into tuples\n",
    "    nltk_formatted_prediction = list(group(listed_ne, 2))\n",
    "    \n",
    "    # Now we can test the accuracy of NLTK:\n",
    "    nltk_accuracy = accuracy(reference_annotations, nltk_formatted_prediction)\n",
    "    print(\"-------------------\")\n",
    "    print(\"NLTK accuracy\")\n",
    "    print(filepath, nltk_accuracy)\n",
    "    \n",
    "    print(\"-------------------\")\n",
    "    print(\"Stanford accuracy\")\n",
    "    stanford_prediction = st.tag(pure_tokens)\n",
    "    stanford_accuracy = accuracy(reference_annotations, stanford_prediction)\n",
    "    print(filepath, stanford_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "nltk_stanfortd_accuracy('eng.testa')\n",
    "nltk_stanfortd_accuracy('eng.testb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque une meilleure performance du côté de Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [FreqDist](https://stackoverflow.com/questions/4634787/freqdist-with-nltk)\n",
    "* [str.contains()](https://stackoverflow.com/questions/3437059/does-python-have-a-string-contains-substring-method)\n",
    "* [stanford_nltk_accuracy](https://pythonprogramming.net/testing-stanford-ner-taggers-for-accuracy/)\n",
    "* [The Place beyond the winds](http://www.gutenberg.org/ebooks/18488?msg=welcome_stranger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
